{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.什么是线性回归\n",
    "\n",
    "- 线性：两个变量之间的关系**是**一次函数关系的——图象**是直线**，叫做线性。\n",
    "- 非线性：两个变量之间的关系**不是**一次函数关系的——图象**不是直线**，叫做非线性。\n",
    "- 回归：人们在测量事物的时候因为客观条件所限，求得的都是测量值，而不是事物真实的值，为了能够得到真实值，无限次的进行测量，最后通过这些测量数据计算**回归到真实值**，这就是回归的由来。\n",
    "\n",
    "## 2. 能够解决什么样的问题\n",
    "\n",
    "对大量的观测数据进行处理，从而得到比较符合事物内部规律的数学表达式。也就是说寻找到数据与数据之间的规律所在，从而就可以模拟出结果，也就是对结果进行预测。解决的就是通过已知的数据得到未知的结果。例如：对房价的预测、判断信用评价、电影票房预估等。\n",
    "\n",
    "## 3. 一般表达式是什么\n",
    "\n",
    "### 3.1 单特征线性回归方程\n",
    "\n",
    "<center>$Y= \\theta_{1}*x + \\theta_{0}$                              \n",
    "    \n",
    "$\\theta_{1}$叫做x的系数，\n",
    "\n",
    "$\\theta_{0}$叫做偏置项。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 多特征线性回归方程：\n",
    "    \n",
    "<center>$Y=\\theta_{n} * x_{n}+ ... \\theta_{i} * x_{i} + \\theta_{1} * x_{1} + \\theta_{0}=\\sum_{i=0}^n\\theta_{i}*x_{i}=\\theta^{T}x$\n",
    "\n",
    "$\\theta_{i}$叫做$x_{i}$的系数，\n",
    "$\\theta_{0}$叫做偏置项。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.误差\n",
    "\n",
    "真实值和预测值之间的差异，肯定是存在的\n",
    "\n",
    "对于每一个样本：真实值=预测值+误差值，转化为公式：$y^{i}=\\theta^{T}x^{i}+\\epsilon^{i}$\n",
    "\n",
    "误差是**独立**并且具有**相同的分布**，并且服从均值为 $\\mu=0$ 方差为 $\\sigma^{2}$ 的高斯分布(正态分布)\n",
    "\n",
    "高斯分布的含义:其概率密度函数为正态分布的期望值$\\mu$ 决定了其位置，其标准差$\\sigma$决定了分布的幅度。\n",
    "\n",
    "单个样本服从高斯分布:\n",
    "\n",
    "![本地路径](./img/01_单个样本的误差值.jpeg)\n",
    "\n",
    "似然函数\n",
    "\n",
    "\n",
    "![本地路径](./img/02_似然函数.jpeg)\n",
    "\n",
    "对数似然函数\n",
    "\n",
    "![本地路径](./img/03_对数似然函数.jpeg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算到最后，目标损失函数是:\n",
    "\n",
    "$ J{(\\theta)}=\\frac{1}{2}\\sum_{i=1}^{m}(y^{i}-\\theta^{T}x^{i})^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.梯度下降\n",
    "\n",
    "### 5.1 正定方程求解\n",
    "![title](./img/线性回归/1.png)\n",
    "\n",
    "当做是一个巧合就可以了，机器学习中核心的思想是**迭代更新**\n",
    "\n",
    "### 5.2 梯度下降（核心解决方案）\n",
    "\n",
    "不光在线性回归中能用上，还有其他算法中能用上，比如神经网络\n",
    "\n",
    "![title](./img/线性回归/2.png)\n",
    "\n",
    "#### 问题：步长太小\n",
    "![title](./img/线性回归/3.png)\n",
    "#### 问题：步长太大\n",
    "![title](./img/线性回归/4.png)\n",
    "\n",
    "### 5.3 批量梯度下降\n",
    "\n",
    "![title](./img/线性回归/7.png)\n",
    "\n",
    "### 5.4 随机梯度下降\n",
    "\n",
    "![title](./img/线性回归/8.png)\n",
    "\n",
    "### 5.5 小批量梯度下降\n",
    "\n",
    "### 5.6 梯度下降法之间的联系与区别\n",
    "\n",
    "## 6.学习率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
